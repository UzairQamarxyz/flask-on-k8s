#+TITLE:        DevSecOps Assessment
#+SUBTITLE:     Quick reference card
#+AUTHOR:       Uzair Qamar
#+EMAIL:        uzairqamarxyz@gmail.com
#+DESCRIPTION:  DevSecOps Assessment Task
#+KEYWORDS:     kubernetes, helm, kubeadm, python, flask, pytest, cicd, kibana, fluentbit, elasticsearch
#+LANGUAGE:     en


** Cluster Setup
We'll be creating a single master/worker node cluster using =kubeadm=. We'll be using =containerd= as our runtime =calico= as our CNI.
*** Pre-requisites
Make sure you have the following packages installed.
  - =kubeadm=
  - =contianerd=
  - =kubectl=

Create the default containerd settings and set =SystemdCgroup= to true
#+begin_src shell
containerd config default | tee /etc/containerd/config.toml > /dev/null
sed -i "s/SystemdCgroup = false/SystemdCgroup = true/g" "/etc/containerd/config.toml"
#+end_src

Make sure =net.ipv4.ip_forward= is enabled.
#+begin_src shell
sudo sysctl net.ipv4.ip_forward=1
#+end_src

If you have swap enabled, then set this in your =/etc/default/kubelet= file.
#+begin_src shell
KUBELET_EXTRA_ARGS=--fail-swap-on=false
#+end_src

*** Create the Cluster
Initialize the cluster with this following command. Change the =pod-network-cidr= if need be.
#+begin_src shell
sudo kubeadm init --pod-network-cidr=192.168.0.0/16
#+end_src

*** Update your kubeconfig
Copy the kubeadm kubeconfig.
#+begin_src shell
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/kubeadm-config
sudo chown $(id -u):$(id -g) $HOME/.kube/kubeadm-config
#+end_src

*** Untaint your node
In order to run workload on your cluster, you need to remove the taints.
#+begin_src shell
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
#+end_src

** Install Calico
*** Pre-requisites
If you're using NetworkManager, then add this to the configuration file.
#+begin_src scheme
# /etc/NetworkManager/conf.d/calico.conf
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali
#+end_src

*** Install Calico via Tigera Operator
We'll be using helm chart to deploy Tigera Operator which will in turn install calico.
#+begin_src shell
helm repo add projectcalico https://docs.tigera.io/calico/charts
helm install calico projectcalico/tigera-operator --version v3.30.3 --namespace tigera-operator --create-namespace
#+end_src

Watch the status of the calico pods
#+begin_src shell
watch kubectl get pods -n calico-system
#+end_src

If the pods are stuck in "ContainerCreating" or "Pending". Make sure the =calico= & =calico-ipam= binaries are also in =/usr/lib/cni/=.
#+begin_src shell
sudo cp /opt/cni/bin/calico /opt/cni/bin/calico-ipam /usr/lib/cni/
#+end_src

*** Cleanup
Here's a cleanup script incase you want to cleanup the kubeadm init.
#+begin_src shell
# Stop kubelet
sudo systemctl stop kubelet

# Delete Calicoâ€™s CNI config & binaries:
sudo rm -rf /etc/cni/net.d/*
sudo rm -rf /opt/cni/bin/*

# Delete leftover Calico state:
sudo rm -rf /var/lib/cni/
sudo rm -rf /var/run/calico
sudo rm -rf /var/lib/calico

# Remove stray network interfaces (tunl0 and cali):*
sudo ip link delete tunl0
# remove each cali* interface
for i in $(ip -br a | grep -o 'cali[a-f0-9]\+'); do
    sudo ip link delete "$i"
done

# Flush iptables rules (Calico sets a lot of them):
sudo iptables -F
sudo iptables -X
sudo iptables -t nat -F
sudo iptables -t nat -X
sudo iptables -t mangle -F
sudo iptables -t mangle -X

# Restart networking (or reboot to be safe):
sudo systemctl restart NetworkManager
sudo systemctl start kubelet
#+end_src

** Install Metrics Server
Let's deploy metrics-server via helm chart.
#+begin_src shell
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm upgrade --install metrics-server metrics-server/metrics-server \
  -n kube-system \
  --set "args[0]=--cert-dir=/tmp" \
  --set "args[1]=--kubelet-preferred-address-types=InternalIP\,ExternalIP\,Hostname" \
  --set "args[2]=--kubelet-use-node-status-port" \
  --set "args[3]=--metric-resolution=15s" \
  --set "args[4]=--kubelet-insecure-tls"
#+end_src

#+begin_quote
[!NOTE]
Note: We're passing the =--kubelet-insecure-tls= else it gives the following error. No other fixes worked for me.
x509: cannot validate certificate for <ip> because it doesn't contain any IP SANs
#+end_quote

** Enable RBAC & Create SA for Deployments
RBAC should already be enabled. Here's a quick check.
#+begin_src shell
kubectl api-versions | grep rbac
#+end_src

Now deploy the RBAC.
#+begin_src shell
kubectl apply -f ./cluster/rbac.yaml
#+end_src

** Generate and store kubeconfig for SA
Use the [[file:scripts/create-kubeconfig.sh][helper script]] to generate the kubeconfig.

** Restrict Access
Use firewall to block access that's the easiest and the safest way

** CI/CD Pipeline
*** Workflow
Here is a general overview of the entire pipeline. The pipeline will run on all pushes and PRs to =main=.
- *sast:* Run =bandit= for static checks.
- *dependency-scan:* Run =pip-audit= for dependency audits.
- *tests:* Run the app_test.py using =pytests=.
- *lint-dockerfile:* Run =hadolint= to lint the dockerfile.
- *build-docker:* Build the docker image.
- *scan-docker:* Scan the docker image for vulnerabilities using =trivy=.
- *push-docker:* Push docker to my public repository.
- *deploy-to-kube:* Deploy the helm chart to the cluster via helm on the self-hosted runner(your machine).

** The Flask Application
*** Overview
Application is a simple web app that fetches random facts from the [[https://uselessfacts.jsph.pl/api/v2/facts/random][random facts api]].

** EFK Stack
*** EKF Auxiliaries Chart
This is a chart that deploys auxiliaries needed for the EFK stack.
These auxiliaries include:
- A namespace called =logging=
- A Default Storage class called =local-storage=
- A Persistent Volume for Elasticsearch called =elasticsearch-pv=
- A Persistent Volume for Kibana called =kibana-pv=

This needs to be deployed first before deploying the stack.

*** Secrets for Elasticsearch and Kibana
Create two secrets in the =logging= namespace.
One called =elasticsearch-secret= which contains the password for the =elastic= user.
#+begin_src shell
kubectl create secret generic elasticsearch-secret \
  --from-literal=elasticsearch-password='password123' \
  --namespace logging
#+end_src

Second called =kibana-secret= which contains the password for the =kibana_system= user.
#+begin_src
kubectl create secret generic kibana-secret \
  --from-literal=kibana-password='password1234' \
  --namespace logging
#+end_src


*** Elasticsearch
Start by creating mount paths for the PV for elasticsearch.
#+begin_src shell
sudo mkdir -p /mnt/es-data
sudo chmod 777 /mnt/es-data # Make it writable by the Elasticsearch pod
#+end_src

Now install the chart.
#+begin_src shell
helm install elasticsearch --namespace logging oci://registry-1.docker.io/bitnamicharts/elasticsearch --create-namespace --values cluster/values-elasticsearch.yaml
#+end_src

#+begin_quote
[!NOTE]
- Password for the =kibana_system= is modified in the =postStart= lifecycle for the elasticsearch chart.
- The master node will act as the data node as well as the ingest node.
#+end_quote

*** Fluentbit
Deploy the fluent-bit helm chart.
#+begin_src shell
helm install fluent-bit --namespace logging oci://registry-1.docker.io/bitnamicharts/fluent-bit --create-namespace --values cluster/values-fluentbit.yaml
#+end_src

#+begin_quote
[!IMPORTANT]
- We had to set security context to enable fluent-bit to read from =/var/logs/containers= folder.
- We are mounting elasticsearch TLS so fluent-bit can authenticate with the elasticsearch server.
#+end_quote

*** Kibana
Start by creating mount paths for the PV for elasticsearch.
#+begin_src shell
sudo mkdir -p /mnt/kibana-data
sudo chmod 777 /mnt/kibana-data # Make it writable by the Kibana pod
#+end_src

Now install the chart.
#+begin_src shell
helm install kibana --namespace logging oci://registry-1.docker.io/bitnamicharts/kibana --create-namespace --values cluster/values-kibana.yaml
#+end_src

** Kibana Access
Once Kibana is up and running, port forward the service and access the dashboard locally via the following command.
#+begin_src shell
echo "Visit http://127.0.0.1:8080 to use your application"
kubectl port-forward -n logging svc/kibana 8080:5601
#+end_src

#+caption: Kibana Dashboard
#+BEGIN_HTML
<img src="./docs/kibana dashboard.png" />
#+END_HTML
