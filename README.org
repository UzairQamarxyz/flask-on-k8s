#+TITLE:        DevSecOps Assessment
#+SUBTITLE:     Quick reference card
#+AUTHOR:       Uzair Qamar
#+EMAIL:        uzairqamarxyz@gmail.com
#+DESCRIPTION:  DevSecOps Assessment Task
#+KEYWORDS:     kubernetes, helm, kubeadm, python, flask, pytest, cicd
#+LANGUAGE:     en


* DevSecOps Assessment

https://docs.tigera.io/calico/latest/getting-started/kubernetes/k8s-single-node
** Step 1: Cluster Setup
*** How to setup the cluster
1. containerd config default | tee /etc/containerd/config.toml > /dev/null
2. sed -i "s/SystemdCgroup = false/SystemdCgroup = true/g" "/etc/containerd/config.toml"
3. Install kubeadm along with containerd runtime https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
   1. What is containerd? Why are there so many options
4. Create a cluster https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
   1. For debian users: https://stackoverflow.com/a/74695838
   2. src_bash[:exports code]{# echo 1 >/proc/sys/net/ipv4/ip_forward} Enable IP Fwd
   3. src_bash[:exports code]{KUBELET_EXTRA_ARGS=--fail-swap-on=false} If you have swap on in your cluster
   4. src_bash[:exports code]{sudo kubeadm init --pod-network-cidr=192.168.0.0/16} init kubeadm
   5. Create a new kube config for kubeadm
      #+begin_src
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/kubeadm-config
sudo chown $(id -u):$(id -g) $HOME/.kube/kubeadm-config
      #+end_src
   6. kubectl taint nodes --all node-role.kubernetes.io/control-plane-

*** Install Calico
https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises
1. Add this to NetworkManager configuration file.
#+begin_src
# /etc/NetworkManager/conf.d/calico.conf
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali
#+end_src
1. Install Tigera Operator
#+begin_src shell
helm repo add projectcalico https://docs.tigera.io/calico/charts
helm install calico projectcalico/tigera-operator --version v3.30.3 --namespace tigera-operator --create-namespace
#+end_src

#+RESULTS:
| projectcalico already exists with the same configuration | skipping |

2. Watch the status of the calico pods
#+begin_src shell
watch kubectl get pods -n calico-system
#+end_src

3. If the pods are stuck in "ContainerCreating" or "Pending". Make sure the =calico= & =calico-ipam= binaries are also in =/usr/lib/cni/=
#+begin_src shell
sudo cp /opt/cni/bin/calico /opt/cni/bin/calico-ipam /usr/lib/cni/
#+end_src

**** Cleanup

#+begin_src shell
# Stop kubelet
sudo systemctl stop kubelet

# Delete Calicoâ€™s CNI config & binaries:
sudo rm -rf /etc/cni/net.d/*
sudo rm -rf /opt/cni/bin/*

# Delete leftover Calico state:
sudo rm -rf /var/lib/cni/
sudo rm -rf /var/run/calico
sudo rm -rf /var/lib/calico

# Remove stray network interfaces (tunl0 and cali):*
sudo ip link delete tunl0
# remove each cali* interface
for i in $(ip link show | grep cali | awk -F: '{print $2}' | xargs); do
  sudo ip link delete $i
done

# Flush iptables rules (Calico sets a lot of them):
sudo iptables -F
sudo iptables -X
sudo iptables -t nat -F
sudo iptables -t nat -X
sudo iptables -t mangle -F
sudo iptables -t mangle -X

# Restart networking (or reboot to be safe):
sudo systemctl restart NetworkManager
sudo systemctl start kubelet
#+end_src

*** Install Metrics Server
Let's use Helm for this
https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs
I ended up doing the insecure tls
#+begin_src shell
#+end_src

#+begin_src shell
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm upgrade --install metrics-server metrics-server/metrics-server \
  -n kube-system \
  --set "args[0]=--cert-dir=/tmp" \
  --set "args[1]=--kubelet-preferred-address-types=InternalIP\,ExternalIP\,Hostname" \
  --set "args[2]=--kubelet-use-node-status-port" \
  --set "args[3]=--metric-resolution=15s" \
  --set "args[4]=--kubelet-insecure-tls"
#+end_src

*** Enable RBAC & Create SA for Deployments
RBAC should already be enabled. Here's a quick check.
#+begin_src shell
kubectl api-versions | grep rbac
#+end_src

For the Service Account, the Role and the RoleBinding. This is namespaced.
#+begin_src yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: dev
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: deployer
  namespace: dev
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: deployer-role
rules:
  - apiGroups: ["apps"]
    resources: ["deployments", "statefulsets", "daemonsets", "replicasets"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: [""]
    resources: ["pods", "pods/log", "services", "configmaps", "secrets", "persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["batch"]
    resources: ["jobs", "cronjobs"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses", "networkpolicies"]
    verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: deployer-binding
  namespace: dev
subjects:
  - kind: ServiceAccount
    name: deployer
    namespace: dev
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: deployer-role
#+end_src

*** Generate and store kubeconfig for SA
Use the [[file:scripts/create-kubeconfig.sh][helper script]] to generate the kubeconfig.

*** Restrict Access
Use firewall to block access that's the easiest and the safest way

** Step 2: CI/CD Pipeline
*** Jobs
- check-paths: Checks whether or not the app or the dockerfile changed
- sast: Run =bandit= for SAST
- dependency-scan: Run =pip-audit= for dependency audits
- tests: Run the app_test.py using =pytests=
- lint-dockerfile: Run =hadolint= to lint the dockerfile
- build-docker: Build the docker image
- scan-docker: Scan the docker image for vulnerabilities using =trivy=
- push-docker: Push docker to my public repository

** Step 3: Deploy Flask App via Helm
*** Application
Application is a simple web app that fetches random facts from the [[https://uselessfacts.jsph.pl/api/v2/facts/random][random facts api]]

Run it on =0.0.0.0= despite bandit giving errors. Reason is that in a bare hosted environment where you're running server directly, it exposes the application to the outside world with no security restrictions. But in kubernetes this is required for applications inside the cluster to talk to one another and security is actually controlled by kubernetes service types and network policies.
https://stackoverflow.com/a/30329547
