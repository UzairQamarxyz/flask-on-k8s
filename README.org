#+TITLE:        DevSecOps Assessment
#+SUBTITLE:     Quick reference card
#+AUTHOR:       Uzair Qamar
#+EMAIL:        uzairqamarxyz@gmail.com
#+DESCRIPTION:  DevSecOps Assessment Task
#+KEYWORDS:     kubernetes, helm, kubeadm, python, flask, pytest, cicd, kibana, fluentbit, elasticsearch
#+LANGUAGE:     en


* DevSecOps Assessment

** Cluster Setup
We'll be creating a single master/worker node cluster using =kubeadm=. We'll be using =containerd= as our runtime =calico= as our CNI.
*** Pre-requisites
Make sure you have the following packages installed.
  - =kubeadm=
  - =contianerd=
  - =kubectl=

Create the default containerd settings and set =SystemdCgroup= to true
#+begin_src shell
containerd config default | tee /etc/containerd/config.toml > /dev/null
sed -i "s/SystemdCgroup = false/SystemdCgroup = true/g" "/etc/containerd/config.toml"
#+end_src

Make sure =net.ipv4.ip_forward= is enabled.
#+begin_src shell
sudo sysctl net.ipv4.ip_forward=1
#+end_src

If you have swap enabled, then set this in your =/etc/default/kubelet= file.
#+begin_src shell
KUBELET_EXTRA_ARGS=--fail-swap-on=false
#+end_src

*** Create the Cluster
Initialize the cluster with this following command. Change the =pod-network-cidr= if need be.
#+begin_src shell
sudo kubeadm init --pod-network-cidr=192.168.0.0/16
#+end_src

*** Update your kubeconfig
Copy the kubeadm kubeconfig.
#+begin_src shell
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/kubeadm-config
sudo chown $(id -u):$(id -g) $HOME/.kube/kubeadm-config
#+end_src

*** Untaint your node
In order to run workload on your cluster, you need to remove the taints.
#+begin_src shell
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
#+end_src

** Install Calico
https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises
*** Pre-requisites
If you're using NetworkManager, then add this to the configuration file.
#+begin_src scheme
# /etc/NetworkManager/conf.d/calico.conf
[keyfile]
unmanaged-devices=interface-name:cali*;interface-name:tunl*;interface-name:vxlan.calico;interface-name:vxlan-v6.calico;interface-name:wireguard.cali;interface-name:wg-v6.cali
#+end_src

*** Install Calico via Tigera Operator
We'll be using helm chart to deploy Tigera Operator which will in turn install calico.
#+begin_src shell
helm repo add projectcalico https://docs.tigera.io/calico/charts
helm install calico projectcalico/tigera-operator --version v3.30.3 --namespace tigera-operator --create-namespace
#+end_src

Watch the status of the calico pods
#+begin_src shell
watch kubectl get pods -n calico-system
#+end_src

If the pods are stuck in "ContainerCreating" or "Pending". Make sure the =calico= & =calico-ipam= binaries are also in =/usr/lib/cni/=.
#+begin_src shell
sudo cp /opt/cni/bin/calico /opt/cni/bin/calico-ipam /usr/lib/cni/
#+end_src

*** Cleanup
Here's a cleanup script incase you want to cleanup the kubeadm init.
#+begin_src shell
# Stop kubelet
sudo systemctl stop kubelet

# Delete Calicoâ€™s CNI config & binaries:
sudo rm -rf /etc/cni/net.d/*
sudo rm -rf /opt/cni/bin/*

# Delete leftover Calico state:
sudo rm -rf /var/lib/cni/
sudo rm -rf /var/run/calico
sudo rm -rf /var/lib/calico

# Remove stray network interfaces (tunl0 and cali):*
sudo ip link delete tunl0
# remove each cali* interface
for i in $(ip -br a | grep -o 'cali[a-f0-9]\+'); do
    sudo ip link delete "$i"
done

# Flush iptables rules (Calico sets a lot of them):
sudo iptables -F
sudo iptables -X
sudo iptables -t nat -F
sudo iptables -t nat -X
sudo iptables -t mangle -F
sudo iptables -t mangle -X

# Restart networking (or reboot to be safe):
sudo systemctl restart NetworkManager
sudo systemctl start kubelet
#+end_src

** Install Metrics Server
Let's deploy metrics-server via helm chart.
#+begin_src shell
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm upgrade --install metrics-server metrics-server/metrics-server \
  -n kube-system \
  --set "args[0]=--cert-dir=/tmp" \
  --set "args[1]=--kubelet-preferred-address-types=InternalIP\,ExternalIP\,Hostname" \
  --set "args[2]=--kubelet-use-node-status-port" \
  --set "args[3]=--metric-resolution=15s" \
  --set "args[4]=--kubelet-insecure-tls"
#+end_src

#+begin_quote
Note: We're passing the =--kubelet-insecure-tls= else it gives the following error. No other fixes worked for me.
x509: cannot validate certificate for <ip> because it doesn't contain any IP SANs
#+end_quote

** Enable RBAC & Create SA for Deployments
RBAC should already be enabled. Here's a quick check.
#+begin_src shell
kubectl api-versions | grep rbac
#+end_src

Now deploy the RBAC.
#+begin_src shell
kubectl apply -f ./cluster/rbac.yaml
#+end_src

** Generate and store kubeconfig for SA
Use the [[file:scripts/create-kubeconfig.sh][helper script]] to generate the kubeconfig.

** Restrict Access
Use firewall to block access that's the easiest and the safest way

** Step 2: CI/CD Pipeline
*** Jobs
- check-paths: Checks whether or not the app or the dockerfile changed
- sast: Run =bandit= for SAST
- dependency-scan: Run =pip-audit= for dependency audits
- tests: Run the app_test.py using =pytests=
- lint-dockerfile: Run =hadolint= to lint the dockerfile
- build-docker: Build the docker image
- scan-docker: Scan the docker image for vulnerabilities using =trivy=
- push-docker: Push docker to my public repository

** Step 3: Deploy Flask App via Helm
*** Application
Application is a simple web app that fetches random facts from the [[https://uselessfacts.jsph.pl/api/v2/facts/random][random facts api]]

Run it on =0.0.0.0= despite bandit giving errors. Reason is that in a bare hosted environment where you're running server directly, it exposes the application to the outside world with no security restrictions. But in kubernetes this is required for applications inside the cluster to talk to one another and security is actually controlled by kubernetes service types and network policies.
https://stackoverflow.com/a/30329547

** Step 4: EFK Stack
*** Fluentbit
helm install fluent-bit --namespace logging oci://registry-1.docker.io/bitnamicharts/fluent-bit --create-namespace --values values-fluentbit.yaml

We had to set security context to enable fluent-bit to read from /var/logs/containers folder
We are mounting elasticsearch TLS so fluent-bit can authenticate with the elasticsearch server

*** Elasticsearch
helm install elasticsearch --namespace logging oci://registry-1.docker.io/bitnamicharts/elasticsearch --create-namespace --values values-elasticsearch.yaml

sudo mkdir -p /mnt/es-data
sudo chmod 777 /mnt/es-data # Make it writable by the Elasticsearch pod

had to add toleration for the controlplane node and node affinity

pass in the password for the user `elastic` through a kubernetes secret
kubectl create secret generic elasticsearch-secret \
  --from-literal=elasticsearch-password='password123' \
  --namespace logging

*** Kibana
helm install kibana --namespace logging oci://registry-1.docker.io/bitnamicharts/kibana --create-namespace --values values-kibana.yaml

kubectl create secret generic kibana-secret \
  --from-literal=kibana-password='password1234' \
  --namespace logging
